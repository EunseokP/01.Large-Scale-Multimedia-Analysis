{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import cPickle\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.cluster.k_means_ import KMeans\n",
    "import sys\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from lightgbm import LGBMClassifier\n",
    "import sys\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['scripts/create_kmeans.py', './kmeans', 100, './surf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = sys.argv[1]+'/knn_{}_model.sav'.format(sys.argv[2])\n",
    "cluster_num = int(sys.argv[2])\n",
    "surf_file_list = sys.argv[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Usage: {0} kmeans_model, cluster_num, file_list\n",
    "# model_path = 'knn_100_model'\n",
    "# n_clusters = 100\n",
    "# video_list = './list/train.video'\n",
    "\n",
    "# kmeans_model = './kmeans/'+model_path+'.sav'; file_list = video_list\n",
    "# cluster_num = int(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(surf_file_list, '*.pkl')\n",
    "\n",
    "filelist = []\n",
    "\n",
    "for file in glob.glob(path):\n",
    "    filelist.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(k, model, path_list, n_frames = 500):\n",
    "    loaded_model = model\n",
    "    k = k\n",
    "    start_time = time.time()\n",
    "#     features_dict = dict()\n",
    "    features_list = []\n",
    "    filelist = path_list\n",
    "    except_list = ['./surf/HVC5510.pkl', './surf/HVC721.pkl', './surf/HVC3124.pkl']\n",
    "#     for i in range(len(filelist)):\n",
    "    for i in range(100):\n",
    "        if filelist[i] in except_list:\n",
    "            array_data = np.zeros((1, 64))\n",
    "        else:\n",
    "            with open(filelist[i], 'rb') as f:\n",
    "                    try:\n",
    "                        data = pickle.load(f)\n",
    "                        if len(data) != 0:\n",
    "                            foo_list = []\n",
    "                            if len(data) >= 500:                            \n",
    "                                rsr_idx = np.random.choice(len(data), n_frames)\n",
    "                                for idx in rsr_idx:\n",
    "                                    if len(data[idx].shape) == 3:\n",
    "                                        foo_list.append(data[idx][0])\n",
    "                                    else:\n",
    "                                        pass\n",
    "                            else:\n",
    "                                for idx in range(len(data)):\n",
    "                                    if len(data[idx].shape) == 3:\n",
    "                                        foo_list.append(data[idx][0])\n",
    "                                    else:\n",
    "                                        pass\n",
    "                        else:\n",
    "                            print('{}th Pickle is Empty!! Skip it!!'.format(i))                    \n",
    "                        array_data = np.concatenate(foo_list, axis = 0)\n",
    "                    except EOFError:\n",
    "        #                     array_data = np.zeros((30000, 64))\n",
    "                        array_data = np.zeros((1, 64))\n",
    "\n",
    "        pred_centers = loaded_model.predict(array_data)\n",
    "        num_clusters = k\n",
    "        bow_preds = np.zeros((1, num_clusters))\n",
    "\n",
    "        for ind in pred_centers:\n",
    "            bow_preds[0, ind] += 1\n",
    "        norm_feat = (1.0 * bow_preds)/np.sum(bow_preds)\n",
    "#         features_dict[i] = norm_feat\n",
    "        features_list.append(norm_feat)\n",
    "#         print(i)\n",
    "        if i % 100 == 0:\n",
    "            print('{}th step is progressing!!!'.format(i))\n",
    "        else: pass\n",
    "    print(\"===== The time consuming of getting features : {} seconds =====\".format((time.time() - start_time)))    \n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foo = get_features(cluster_num, model, filelist, n_frames = 100)\n",
    "# data_array = np.concatenate(foo, axis = 0)\n",
    "# data_mart = pd.DataFrame(data_array)\n",
    "# data_mart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open(filelist[0], 'rb') as f:\n",
    "# #     data = pickle.load(f)\n",
    "foo = get_features(100, model, filelist, n_frames = 100)\n",
    "data_array = np.concatenate(foo, axis = 0)\n",
    "data_mart = pd.DataFrame(data_array)\n",
    "data_mart.to_csv('./total_surf_features_k{}.csv'.format(cluster_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the kmeans model\n",
    "# kmeans_model = './{}.sav'.format(sys.argv[1])\n",
    "model = pickle.load(open(kmeans_model,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = pd.read_csv('./total_surf_features_k{}.csv'.format(cluster_num))\n",
    "total_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name_ind = []\n",
    "for i in range(len(filelist)):\n",
    "    match_front = re.search('surf/', filelist[i])\n",
    "    match_end = re.search('.pkl', filelist[i])\n",
    "    video_name_ind.append(filelist[i][match_front.end():match_end.start()])\n",
    "    video_name = pd.DataFrame({'video': video_name_ind})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = cluster_num\n",
    "column_name = ['video']\n",
    "for i in range(k):\n",
    "    column_name.append('feature_{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([video_name, total_features], axis = 1)\n",
    "total_data.columns = column_name\n",
    "train_ind = pd.read_csv('./list/train', sep = ' ', header = None)\n",
    "valid_ind = pd.read_csv('./list/val', sep = ' ', header = None)\n",
    "test_ind = pd.read_csv('./list/test.video', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind['Data'] = 'TRAIN'\n",
    "valid_ind['Data'] = 'VALID'\n",
    "test_ind[1] = 'UNK'\n",
    "test_ind['Data'] = 'TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind.columns = ['video','target','Data']\n",
    "valid_ind.columns = ['video','target','Data']\n",
    "test_ind.columns = ['video','target','Data']\n",
    "data_lable = pd.concat([train_ind, valid_ind, test_ind], axis = 0).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_lable['target_p001'] = \n",
    "data_lable['target_p001'] = data_lable['target']\n",
    "data_lable['target_p002'] = data_lable['target']\n",
    "data_lable['target_p003'] = data_lable['target']\n",
    "data_lable['target_p001_10'] = 1\n",
    "data_lable['target_p002_10'] = 1\n",
    "data_lable['target_p003_10'] = 1\n",
    "\n",
    "data_lable['target_p001'][data_lable['target'] != 'P001'] = 'Other'\n",
    "data_lable['target_p002'][data_lable['target'] != 'P002'] = 'Other'\n",
    "data_lable['target_p003'][data_lable['target'] != 'P003'] = 'Other'\n",
    "data_lable['target_p001_10'][data_lable['target'] != 'P001'] = 0\n",
    "data_lable['target_p002_10'][data_lable['target'] != 'P002'] = 0\n",
    "data_lable['target_p003_10'][data_lable['target'] != 'P003'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mart = total_data.merge(data_lable, how = 'right', on = 'video')\n",
    "total_mart = total_mart.fillna(0)\n",
    "\n",
    "train_mart = total_mart[total_mart['Data'] == 'TRAIN']\n",
    "valid_mart = total_mart[total_mart['Data'] == 'VALID']\n",
    "test_mart  = total_mart[total_mart['Data'] == 'TEST']\n",
    "\n",
    "total_mart.to_csv('./datamart_surf_total_k{}.csv'.format(cluster_num), index=False)\n",
    "train_mart.to_csv('./datamart_surf_train_k{}.csv'.format(cluster_num), index=False)\n",
    "valid_mart.to_csv('./datamart_surf_valid_k{}.csv'.format(cluster_num), index=False)\n",
    "test_mart.to_csv('./datamart_surf_test_k{}.csv'.format(cluster_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate k-means features for videos; each video is represented by a single vector\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 4:\n",
    "        print \"Usage: {0} kmeans_model, cluster_num, file_list\".format(sys.argv[0])\n",
    "        print \"kmeans_model -- path to the kmeans model\"\n",
    "        print \"cluster_num -- number of cluster\"\n",
    "        print \"file_list -- the list of videos\"\n",
    "        exit(1)\n",
    "    warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "    kmeans_model = './'+sys.argv[1]+'.sav'; file_list = sys.argv[3]\n",
    "    cluster_num = int(sys.argv[2])\n",
    "\n",
    "    # load the kmeans model\n",
    "    # kmeans_model = './{}.sav'.format(sys.argv[1])\n",
    "    kmeans = pickle.load(open(kmeans_model,\"rb\"))\n",
    "    model = kmeans\n",
    "    \n",
    "    # path = './hw1_git/11775-hws/videos/*.mp4'\n",
    "    path = './mfcc/*.csv'\n",
    "\n",
    "    filelist = []\n",
    "\n",
    "    for file in glob.glob(path):\n",
    "        filelist.append(file)\n",
    "        \n",
    "    def get_features(k, model, path_list):\n",
    "        loaded_model= model\n",
    "        start_time = time.time()\n",
    "        features_dict = dict()\n",
    "        filelist = path_list\n",
    "        for i in range(len(filelist)):\n",
    "    #     for i in range(10):        \n",
    "            if i % 1000 == 0: \n",
    "                print('{}th step progressing....'.format(i)) \n",
    "            else: \n",
    "                pass\n",
    "            data = pd.read_csv(filelist[i], sep = ';', header = None)\n",
    "            pred_centers = loaded_model.predict(data)\n",
    "            num_clusters = k\n",
    "            bow_preds = np.zeros((1, num_clusters))\n",
    "\n",
    "            for ind in pred_centers:\n",
    "                bow_preds[0, ind] += 1\n",
    "            norm_feat = (1.0 * bow_preds)/np.sum(bow_preds)\n",
    "            features_dict[i] = pd.DataFrame(norm_feat)\n",
    "\n",
    "        features_total = features_dict[0].copy()\n",
    "        for i in range(1, len(features_dict)):\n",
    "            foo = features_dict[i].copy()\n",
    "            features_total = pd.concat([features_total, foo], axis = 0)\n",
    "            features_total = features_total.reset_index().drop('index', axis = 1)\n",
    "\n",
    "        print(\"===== The time consuming of getting features : {} seconds =====\".format((time.time() - start_time)))\n",
    "        return features_total  \n",
    "    \n",
    "    total_features = get_features(k = cluster_num, model = model, path_list = filelist)    \n",
    "    total_features.to_csv('./total_features_k{}.csv'.format(cluster_num), index=False)\n",
    "    \n",
    "    total_features = pd.read_csv('./total_features_k{}.csv'.format(cluster_num))\n",
    "    total_features.head(3)\n",
    "\n",
    "    video_name_ind = []\n",
    "    for i in range(len(filelist)):\n",
    "        match_front = re.search('mfcc/', filelist[i])\n",
    "        match_end = re.search('.mfcc.csv', filelist[i])\n",
    "        video_name_ind.append(filelist[i][match_front.end():match_end.start()])\n",
    "        video_name = pd.DataFrame({'video': video_name_ind})    \n",
    "    \n",
    "    # Making features columns\n",
    "    k = cluster_num\n",
    "    column_name = ['video']\n",
    "    for i in range(k):\n",
    "        column_name.append('feature_{}'.format(i))\n",
    "\n",
    "    total_data = pd.concat([video_name, total_features], axis = 1)\n",
    "    total_data.columns = column_name\n",
    "\n",
    "    train_ind = pd.read_csv('./list/train', sep = ' ', header = None)\n",
    "    valid_ind = pd.read_csv('./list/val', sep = ' ', header = None)\n",
    "    test_ind = pd.read_csv('./list/test.video', sep = ' ', header = None)\n",
    "\n",
    "    train_ind['Data'] = 'TRAIN'\n",
    "    valid_ind['Data'] = 'VALID'\n",
    "    test_ind[1] = 'UNK'\n",
    "    test_ind['Data'] = 'TEST'\n",
    "\n",
    "    train_ind.columns = ['video','target','Data']\n",
    "    valid_ind.columns = ['video','target','Data']\n",
    "    test_ind.columns = ['video','target','Data']\n",
    "\n",
    "    data_lable = pd.concat([train_ind, valid_ind, test_ind], axis = 0).reset_index().drop('index', axis = 1)\n",
    "    # data_lable['target_p001'] = \n",
    "    data_lable['target_p001'] = data_lable['target']\n",
    "    data_lable['target_p002'] = data_lable['target']\n",
    "    data_lable['target_p003'] = data_lable['target']\n",
    "    data_lable['target_p001_10'] = 1\n",
    "    data_lable['target_p002_10'] = 1\n",
    "    data_lable['target_p003_10'] = 1\n",
    "\n",
    "    data_lable['target_p001'][data_lable['target'] != 'P001'] = 'Other'\n",
    "    data_lable['target_p002'][data_lable['target'] != 'P002'] = 'Other'\n",
    "    data_lable['target_p003'][data_lable['target'] != 'P003'] = 'Other'\n",
    "    data_lable['target_p001_10'][data_lable['target'] != 'P001'] = 0\n",
    "    data_lable['target_p002_10'][data_lable['target'] != 'P002'] = 0\n",
    "    data_lable['target_p003_10'][data_lable['target'] != 'P003'] = 0\n",
    "\n",
    "    total_mart = total_data.merge(data_lable, how = 'right', on = 'video')\n",
    "    total_mart = total_mart.fillna(0)\n",
    "    \n",
    "    train_mart = total_mart[total_mart['Data'] == 'TRAIN']\n",
    "    valid_mart = total_mart[total_mart['Data'] == 'VALID']\n",
    "    test_mart  = total_mart[total_mart['Data'] == 'TEST']\n",
    "    \n",
    "    total_mart.to_csv('./datamart_total_k{}.csv'.format(cluster_num), index=False)\n",
    "    train_mart.to_csv('./datamart_train_k{}.csv'.format(cluster_num), index=False)\n",
    "    valid_mart.to_csv('./datamart_valid_k{}.csv'.format(cluster_num), index=False)\n",
    "    test_mart.to_csv('./datamart_test_k{}.csv'.format(cluster_num), index=False)\n",
    "    print \"K-means features generated successfully!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_ap_SVM(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = SVC(kernel=chi2_kernel, probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of SVM Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_AdaB(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of AdaBoosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_Boost(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of Boosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_xgb(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of XgBoosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_lgbm(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = LGBMClassifier(random_state=0, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of LightGBM Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['scripts/train_model.py', 'P001', 100, './surf_model', 'surf']\n",
    "event_name = sys.argv[1]\n",
    "feat_dim = sys.argv[2]\n",
    "output_file = sys.argv[3]\n",
    "model_type = sys.argv[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mart = pd.read_csv('./datamart_{}_train_k{}.csv'.format(model_type, feat_dim))\n",
    "valid_mart = pd.read_csv('./datamart_{}_valid_k{}.csv'.format(model_type, feat_dim))\n",
    "test_mart  = pd.read_csv('./datamart_{}_test_k{}.csv'.format(model_type, feat_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose for each model based on MAP\n",
    "Xgb_results_p001, _, surf_model_p001 = modeling_ap_xgb(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Xgb_results_p002, _, surf_model_p002 = modeling_ap_xgb(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "LGBM_results_p003, _, surf_model_p003 = modeling_ap_lgbm(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(output_file, event_name+'_'+model_type+'.model.sav')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(surf_model_p001, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python2 scripts/test_svm.py mfcc_pred/$event.model \"kmeans/\" $feat_dim_mfcc mfcc_pred/${event}_mfcc.lst || exit 1;\n",
    "# sys.argv = ['scripts/test_model.py', 'P001', 'cnn_model', 1280, 'cnn_pred/P001_cnn.list', 'cnn']\n",
    "sys.argv = ['scripts/test_model.py', 'P001', 'surf_model', 100, 'surf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: {0} model_file feat_dir feat_dim output_file\".format(sys.argv[0]))\n",
    "        print(\"model_file -- path of the trained svm file\")\n",
    "        print(\"feat_dir -- dir of feature files\")\n",
    "        print(\"output_file -- path to save the prediction score\")\n",
    "        print(\"model_type -- mfcc or asrs\")\n",
    "        exit(1)\n",
    "\n",
    "    event_name = sys.argv[1]\n",
    "    model_file = sys.argv[2]\n",
    "    feat_dim = int(sys.argv[3])\n",
    "    model_type = sys.argv[4]\n",
    "    print(event_name)\n",
    "    print(model_file)\n",
    "    print(feat_dim)\n",
    "    print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_file, event_name+'_'+model_type+'.model.sav')\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "test_mart  = pd.read_csv('./datamart_surf_test_k{}.csv'.format(feat_dim))\n",
    "X_test = test_mart.iloc[:, 1:feat_dim + 1]\n",
    "scores = model.predict_proba(X_test)\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.columns = ['N', 'Y']\n",
    "scores_total = pd.concat([test_mart, scores_df], axis = 1)\n",
    "# scores_total.head(3)\n",
    "\n",
    "test_list = pd.read_csv('./list/test.video', header = None)\n",
    "test_list.columns = ['video']\n",
    "\n",
    "output_file = 'surf_pred/'+event_name+'_'+model_type+'.list'\n",
    "final_list = test_list.merge(scores_total, how = 'left', on = 'video')[['video', 'Y']]\n",
    "# final_list.to_csv(output_file+'.video'+'.csv', index = False)\n",
    "# final_list['Y'].to_csv(output_file+'.csv', index = False, header = True)\n",
    "final_list.to_csv(output_file+'.video'+'.txt', index = False, header = False)\n",
    "final_list['Y'].to_csv(output_file+'.txt', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_results_p001 = modeling_ap_SVM(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "SVM_results_p002 = modeling_ap_SVM(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "SVM_results_p003 = modeling_ap_SVM(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaB_results_p001, _ = modeling_ap_AdaB(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "AdaB_results_p002, _ = modeling_ap_AdaB(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "AdaB_results_p003, _ = modeling_ap_AdaB(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boost_results_p001, _ = modeling_ap_Boost(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Boost_results_p002, _ = modeling_ap_Boost(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Boost_results_p003, _ = modeling_ap_Boost(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgb_results_p001, _ = modeling_ap_xgb(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Xgb_results_p002, _ = modeling_ap_xgb(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Xgb_results_p003, _ = modeling_ap_xgb(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_results_p001, _ = modeling_ap_lgbm(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "LGBM_results_p002, _ = modeling_ap_lgbm(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "LGBM_results_p003, _ = modeling_ap_lgbm(k=100, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
