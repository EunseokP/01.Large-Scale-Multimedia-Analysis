{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "# import cPickle\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.cluster.k_means_ import KMeans\n",
    "import sys\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from lightgbm import LGBMClassifier\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usage: {0} kmeans_model, cluster_num, file_list\n",
    "n_clusters = 50\n",
    "model_path = 'knn_cnn_{}_model'.format(n_clusters)\n",
    "video_list = './list/train.video'\n",
    "\n",
    "kmeans_model = './kmeans/'+model_path+'.sav'; file_list = video_list\n",
    "cluster_num = int(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(kmeans_model,\"rb\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cnn/*.pkl'\n",
    "\n",
    "filelist = []\n",
    "\n",
    "for file in glob.glob(path):\n",
    "    filelist.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(k, model, path_list, n_frames = 500):\n",
    "    loaded_model = model\n",
    "    k = k\n",
    "    start_time = time.time()\n",
    "# sample_dict = dict()\n",
    "    features_list = []\n",
    "    start_time = time.time()\n",
    "    for i in range(len(filelist)):\n",
    "    # for i in range(2500, 20000):\n",
    "        with open(filelist[i], 'rb') as f:\n",
    "            try:\n",
    "                data = pickle.load(f)\n",
    "                if len(data) != 0:\n",
    "                    list_foo = []\n",
    "                    foo = pickle.load(open(filelist[i],\"rb\"))\n",
    "    #                 if len(foo) >= 20:\n",
    "    #                     srs_idx = np.random.choice(len(foo), 10) # simple random sampling\n",
    "    #                 else:\n",
    "    #                     srs_idx = list(range(len(foo)))\n",
    "                    for idx in range(len(foo)):\n",
    "    #                 for idx in srs_idx:\n",
    "                        list_foo.append(foo[idx].detach().numpy())\n",
    "                    foo_concat = np.concatenate(list_foo, axis = 0)\n",
    "                else:\n",
    "                    foo_concat = np.zeros((1,1280))\n",
    "            except EOFError:\n",
    "                pass\n",
    "        pred_centers = loaded_model.predict(foo_concat)\n",
    "        num_clusters = k\n",
    "        bow_preds = np.zeros((1, num_clusters))\n",
    "\n",
    "        for ind in pred_centers:\n",
    "            bow_preds[0, ind] += 1\n",
    "        norm_feat = (1.0 * bow_preds)/np.sum(bow_preds)    \n",
    "        features_list.append(norm_feat)    \n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('========== {}th step is completed! =========='.format(i))\n",
    "        else:\n",
    "            pass\n",
    "    print('========== All complete! It takes {} secs. =========='.format(time.time() - start_time))    \n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = get_features(100, model, filelist, n_frames = 100)\n",
    "foo = get_features(50, model, filelist, n_frames = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open(filelist[0], 'rb') as f:\n",
    "# #     data = pickle.load(f)\n",
    "# foo = get_features(100, model, filelist, n_frames = 100)\n",
    "data_array = np.concatenate(foo, axis = 0)\n",
    "data_mart = pd.DataFrame(data_array)\n",
    "data_mart.to_csv('./total_cnn_features_k{}.csv'.format(n_clusters), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = pd.read_csv('./total_cnn_features_k{}.csv'.format(n_clusters))\n",
    "total_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name_ind = []\n",
    "for i in range(len(filelist)):\n",
    "    match_front = re.search('cnn/', filelist[i])\n",
    "    match_end = re.search('.pkl', filelist[i])\n",
    "    video_name_ind.append(filelist[i][match_front.end():match_end.start()])\n",
    "    video_name = pd.DataFrame({'video': video_name_ind})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = n_clusters\n",
    "column_name = ['video']\n",
    "for i in range(k):\n",
    "    column_name.append('feature_{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([video_name, total_features], axis = 1)\n",
    "total_data.columns = column_name\n",
    "train_ind = pd.read_csv('./list/train', sep = ' ', header = None)\n",
    "valid_ind = pd.read_csv('./list/val', sep = ' ', header = None)\n",
    "test_ind = pd.read_csv('./list/test.video', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind['Data'] = 'TRAIN'\n",
    "valid_ind['Data'] = 'VALID'\n",
    "test_ind[1] = 'UNK'\n",
    "test_ind['Data'] = 'TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind.columns = ['video','target','Data']\n",
    "valid_ind.columns = ['video','target','Data']\n",
    "test_ind.columns = ['video','target','Data']\n",
    "data_lable = pd.concat([train_ind, valid_ind, test_ind], axis = 0).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_lable['target_p001'] = \n",
    "data_lable['target_p001'] = data_lable['target']\n",
    "data_lable['target_p002'] = data_lable['target']\n",
    "data_lable['target_p003'] = data_lable['target']\n",
    "data_lable['target_p001_10'] = 1\n",
    "data_lable['target_p002_10'] = 1\n",
    "data_lable['target_p003_10'] = 1\n",
    "\n",
    "data_lable['target_p001'][data_lable['target'] != 'P001'] = 'Other'\n",
    "data_lable['target_p002'][data_lable['target'] != 'P002'] = 'Other'\n",
    "data_lable['target_p003'][data_lable['target'] != 'P003'] = 'Other'\n",
    "data_lable['target_p001_10'][data_lable['target'] != 'P001'] = 0\n",
    "data_lable['target_p002_10'][data_lable['target'] != 'P002'] = 0\n",
    "data_lable['target_p003_10'][data_lable['target'] != 'P003'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mart = total_data.merge(data_lable, how = 'right', on = 'video')\n",
    "total_mart = total_mart.fillna(0)\n",
    "\n",
    "train_mart = total_mart[total_mart['Data'] == 'TRAIN']\n",
    "valid_mart = total_mart[total_mart['Data'] == 'VALID']\n",
    "test_mart  = total_mart[total_mart['Data'] == 'TEST']\n",
    "\n",
    "total_mart.to_csv('./datamart_cnn_total_k{}.csv'.format(n_clusters), index=False)\n",
    "train_mart.to_csv('./datamart_cnn_train_k{}.csv'.format(n_clusters), index=False)\n",
    "valid_mart.to_csv('./datamart_cnn_valid_k{}.csv'.format(n_clusters), index=False)\n",
    "test_mart.to_csv('./datamart_cnn_test_k{}.csv'.format(n_clusters), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_ap_SVM(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = SVC(kernel=chi2_kernel, probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of SVM Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs\n",
    "\n",
    "def modeling_ap_AdaB(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of AdaBoosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs\n",
    "\n",
    "def modeling_ap_Boost(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of Boosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs\n",
    "\n",
    "def modeling_ap_xgb(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of XgBoosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs\n",
    "\n",
    "def modeling_ap_lgbm(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = LGBMClassifier(random_state=0, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of LightGBM Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose for each model based on MAP\n",
    "Xgb_results_p001, _ = modeling_ap_xgb(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Xgb_results_p002, _ = modeling_ap_xgb(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "LGBM_results_p003, _ = modeling_ap_lgbm(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_results_p001 = modeling_ap_SVM(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "SVM_results_p002 = modeling_ap_SVM(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "SVM_results_p003 = modeling_ap_SVM(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaB_results_p001, _ = modeling_ap_AdaB(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "AdaB_results_p002, _ = modeling_ap_AdaB(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "AdaB_results_p003, _ = modeling_ap_AdaB(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boost_results_p001, _ = modeling_ap_Boost(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Boost_results_p002, _ = modeling_ap_Boost(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Boost_results_p003, _ = modeling_ap_Boost(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgb_results_p001, _ = modeling_ap_xgb(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Xgb_results_p002, _ = modeling_ap_xgb(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Xgb_results_p003, _ = modeling_ap_xgb(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_results_p001, _ = modeling_ap_lgbm(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "LGBM_results_p002, _ = modeling_ap_lgbm(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "LGBM_results_p003, _ = modeling_ap_lgbm(k=n_clusters, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
