{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "# import cPickle\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.cluster.k_means_ import KMeans\n",
    "import sys\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from lightgbm import LGBMClassifier\n",
    "import sys\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['scripts/create_feat_cnn.py', './cnn', 1280]\n",
    "cnn_file_list = sys.argv[1]\n",
    "n_features = sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(cnn_file_list, '*.pkl')\n",
    "\n",
    "filelist = []\n",
    "\n",
    "for file in glob.glob(path):\n",
    "    filelist.append(file)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=800\n",
    "# foo = pickle.load(open(filelist[i],\"rb\"))\n",
    "# list_foo = []\n",
    "# foo_concat = []\n",
    "# for idx in range(len(foo)):\n",
    "#     list_foo.append(foo[idx].detach().numpy())\n",
    "# foo_concat = np.concatenate(list_foo, axis = 0)    \n",
    "# foo_concat\n",
    "# # foo_concat.mean(axis = 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo_concat.mean(axis = 0).reshape(1, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=800\n",
    "# fooo = []\n",
    "# for i in range(len(filelist)):\n",
    "#     foo = pickle.load(open(filelist[i],\"rb\"))\n",
    "#     fooo.append(foo.shape[0])\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)\n",
    "#     else: pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=2129\n",
    "# list_foo = []\n",
    "# foo = pickle.load(open(filelist[i],\"rb\"))\n",
    "# for idx in range(len(foo)):\n",
    "#     list_foo.append(foo[idx].detach().numpy())\n",
    "#     foo_concat = np.concatenate(list_foo, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_dict = dict()\n",
    "knn_data_list = []\n",
    "start_time = time.time()\n",
    "for i in range(len(filelist)):\n",
    "# for i in range(2500, 20000):\n",
    "    with open(filelist[i], 'rb') as f:\n",
    "        try:\n",
    "            data = pickle.load(f)\n",
    "            if len(data) != 0:\n",
    "                list_foo = []\n",
    "                foo = pickle.load(open(filelist[i],\"rb\"))\n",
    "                for idx in range(len(foo)):\n",
    "#                 for idx in srs_idx:\n",
    "                    list_foo.append(foo[idx].detach().numpy())\n",
    "                foo_concat = np.concatenate(list_foo, axis = 0)\n",
    "                knn_data_list.append(foo_concat.mean(axis = 0).reshape(1, n_features))\n",
    "            else:\n",
    "                foo_concat = np.zeros((1,1280))\n",
    "                knn_data_list.append(foo_concat)\n",
    "        except EOFError:\n",
    "            pass\n",
    "    if i % 500 == 0:\n",
    "        print('========== {}th step is completed! =========='.format(i))\n",
    "    else:\n",
    "        pass\n",
    "print('========== All complete! It takes {} secs. =========='.format(time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mart = np.concatenate(knn_data_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mart = pd.DataFrame(knn_mart)\n",
    "data_mart.to_csv('./total_cnn_features_avgpool.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = pd.read_csv('./total_cnn_features_avgpool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name_ind = []\n",
    "for i in range(len(filelist)):\n",
    "    match_front = re.search('cnn/', filelist[i])\n",
    "    match_end = re.search('.pkl', filelist[i])\n",
    "    video_name_ind.append(filelist[i][match_front.end():match_end.start()])\n",
    "    video_name = pd.DataFrame({'video': video_name_ind})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = n_clusters\n",
    "# k = total_features.shape[1]\n",
    "k = n_features\n",
    "column_name = ['video']\n",
    "for i in range(k):\n",
    "    column_name.append('feature_{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([video_name, total_features], axis = 1)\n",
    "total_data.columns = column_name\n",
    "train_ind = pd.read_csv('./list/train', sep = ' ', header = None)\n",
    "valid_ind = pd.read_csv('./list/val', sep = ' ', header = None)\n",
    "test_ind = pd.read_csv('./list/test.video', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind['Data'] = 'TRAIN'\n",
    "valid_ind['Data'] = 'VALID'\n",
    "test_ind[1] = 'UNK'\n",
    "test_ind['Data'] = 'TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind.columns = ['video','target','Data']\n",
    "valid_ind.columns = ['video','target','Data']\n",
    "test_ind.columns = ['video','target','Data']\n",
    "data_lable = pd.concat([train_ind, valid_ind, test_ind], axis = 0).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_lable['target_p001'] = \n",
    "data_lable['target_p001'] = data_lable['target']\n",
    "data_lable['target_p002'] = data_lable['target']\n",
    "data_lable['target_p003'] = data_lable['target']\n",
    "data_lable['target_p001_10'] = 1\n",
    "data_lable['target_p002_10'] = 1\n",
    "data_lable['target_p003_10'] = 1\n",
    "\n",
    "data_lable['target_p001'][data_lable['target'] != 'P001'] = 'Other'\n",
    "data_lable['target_p002'][data_lable['target'] != 'P002'] = 'Other'\n",
    "data_lable['target_p003'][data_lable['target'] != 'P003'] = 'Other'\n",
    "data_lable['target_p001_10'][data_lable['target'] != 'P001'] = 0\n",
    "data_lable['target_p002_10'][data_lable['target'] != 'P002'] = 0\n",
    "data_lable['target_p003_10'][data_lable['target'] != 'P003'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mart = total_data.merge(data_lable, how = 'right', on = 'video')\n",
    "total_mart = total_mart.fillna(0)\n",
    "\n",
    "train_mart = total_mart[total_mart['Data'] == 'TRAIN']\n",
    "valid_mart = total_mart[total_mart['Data'] == 'VALID']\n",
    "test_mart  = total_mart[total_mart['Data'] == 'TEST']\n",
    "\n",
    "total_mart.to_csv('./datamart_cnn_total_avgpool.csv', index=False)\n",
    "train_mart.to_csv('./datamart_cnn_train_avgpool.csv', index=False)\n",
    "valid_mart.to_csv('./datamart_cnn_valid_avgpool.csv', index=False)\n",
    "test_mart.to_csv('./datamart_cnn_test_avgpool.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_ap_SVM(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = SVC(kernel=chi2_kernel, probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of SVM Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_AdaB(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of AdaBoosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_Boost(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=200, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of Boosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_xgb(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of XgBoosting Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model\n",
    "\n",
    "def modeling_ap_lgbm(k, train_data, valid_data, target = 'target_p001_10'):\n",
    "    start_time = time.time()\n",
    "    k = k\n",
    "    train_mart = train_data\n",
    "    valid_mart = valid_data\n",
    "    target = target\n",
    "    \n",
    "    X_train = train_mart.iloc[:,1:k+1]\n",
    "    y_train = train_mart[target]\n",
    "    X_valid = valid_mart.iloc[:,1:k+1]\n",
    "    y_valid = valid_mart[target]\n",
    "    \n",
    "    model = LGBMClassifier(random_state=0, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_valid)\n",
    "    y_probs = model.predict_proba(X_valid)\n",
    "    results = average_precision_score(y_true=y_valid.values, y_score=y_probs[:,1])\n",
    "    print(\"===== The time consuming of LightGBM Modeling : {} seconds =====\".format((time.time() - start_time)))   \n",
    "    print(results)\n",
    "    return results, y_probs, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['scripts/train_model.py', 'P001', 1280, './cnn_model', 'cnn']\n",
    "event_name = sys.argv[1]\n",
    "feat_dim = sys.argv[2]\n",
    "output_file = sys.argv[3]\n",
    "model_type = sys.argv[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mart = pd.read_csv('./datamart_cnn_train_avgpool.csv')\n",
    "valid_mart = pd.read_csv('./datamart_cnn_valid_avgpool.csv')\n",
    "test_mart  = pd.read_csv('./datamart_cnn_test_avgpool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose for each model based on MAP\n",
    "#k = total_features.shape[1]\n",
    "SVM_results_p001, _, cnn_model_p001 = modeling_ap_SVM(k=feat_dim, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "SVM_results_p002, _, cnn_model_p002 = modeling_ap_SVM(k=feat_dim, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Xgb_results_p003, _, cnn_model_p003 = modeling_ap_xgb(k=feat_dim, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(output_file, event_name+'_'+model_type+'.model.sav')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cnn_model_p001, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = output_file = 'cnn_model/P001_cnn.model'\n",
    "filename = output_file+'.sav'\n",
    "pickle.dump(cnn_model_p001, open(filename, 'wb'))\n",
    "\n",
    "output_file = output_file = 'cnn_model/P002_cnn.model'\n",
    "filename = output_file+'.sav'\n",
    "pickle.dump(cnn_model_p002, open(filename, 'wb'))\n",
    "\n",
    "output_file = output_file = 'cnn_model/P003_cnn.model'\n",
    "filename = output_file+'.sav'\n",
    "pickle.dump(cnn_model_p003, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mart.shape[1]\n",
    "test_mart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python2 scripts/test_svm.py mfcc_pred/$event.model \"kmeans/\" $feat_dim_mfcc mfcc_pred/${event}_mfcc.lst || exit 1;\n",
    "# sys.argv = ['scripts/test_model.py', 'P001', 'cnn_model', 1280, 'cnn_pred/P001_cnn.list', 'cnn']\n",
    "sys.argv = ['scripts/test_model.py', 'P001', 'cnn_model', 1280, 'cnn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: {0} model_file feat_dir feat_dim output_file\".format(sys.argv[0]))\n",
    "        print(\"model_file -- path of the trained svm file\")\n",
    "        print(\"feat_dir -- dir of feature files\")\n",
    "        print(\"output_file -- path to save the prediction score\")\n",
    "        print(\"model_type -- mfcc or asrs\")\n",
    "        exit(1)\n",
    "\n",
    "    event_name = sys.argv[1]\n",
    "    model_file = sys.argv[2]\n",
    "    feat_dim = int(sys.argv[3])\n",
    "    model_type = sys.argv[4]\n",
    "    print(event_name)\n",
    "    print(model_file)\n",
    "    print(feat_dim)\n",
    "    print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.shape_fit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_file, event_name+'_'+model_type+'.model.sav')\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "test_mart  = pd.read_csv('./datamart_cnn_test_avgpool.csv')\n",
    "X_test = test_mart.iloc[:, 1:feat_dim + 1]\n",
    "scores = model.predict_proba(X_test)\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.columns = ['N', 'Y']\n",
    "scores_total = pd.concat([test_mart, scores_df], axis = 1)\n",
    "# scores_total.head(3)\n",
    "\n",
    "test_list = pd.read_csv('./list/test.video', header = None)\n",
    "test_list.columns = ['video']\n",
    "\n",
    "output_file = 'cnn_pred/'+event_name+'_'+model_type+'.list'\n",
    "final_list = test_list.merge(scores_total, how = 'left', on = 'video')[['video', 'Y']]\n",
    "# final_list.to_csv(output_file+'.video'+'.csv', index = False)\n",
    "# final_list['Y'].to_csv(output_file+'.csv', index = False, header = True)\n",
    "final_list.to_csv(output_file+'.video'+'.txt', index = False, header = False)\n",
    "final_list['Y'].to_csv(output_file+'.txt', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_results_p001, _, cnn_model_p001 = modeling_ap_SVM(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "SVM_results_p002, _, cnn_model_p001 = modeling_ap_SVM(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "SVM_results_p003, _, cnn_model_p001 = modeling_ap_SVM(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaB_results_p001, _, cnn_model_p001 = modeling_ap_AdaB(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "AdaB_results_p002, _, cnn_model_p001 = modeling_ap_AdaB(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "AdaB_results_p003, _, cnn_model_p001 = modeling_ap_AdaB(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boost_results_p001, _, cnn_model_p001 = modeling_ap_Boost(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Boost_results_p002, _, cnn_model_p001 = modeling_ap_Boost(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Boost_results_p003, _, cnn_model_p001 = modeling_ap_Boost(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgb_results_p001, _, cnn_model_p001 = modeling_ap_xgb(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "Xgb_results_p002, _, cnn_model_p001 = modeling_ap_xgb(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "Xgb_results_p003, _, cnn_model_p001 = modeling_ap_xgb(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_results_p001, _, cnn_model_p001 = modeling_ap_lgbm(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p001_10')\n",
    "LGBM_results_p002, _, cnn_model_p001 = modeling_ap_lgbm(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p002_10')\n",
    "LGBM_results_p003, _, cnn_model_p001 = modeling_ap_lgbm(k=k, train_data = train_mart, valid_data = valid_mart, target = 'target_p003_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
